[
  {
    "objectID": "posts/Blog6/index.html",
    "href": "posts/Blog6/index.html",
    "title": "Credit Card Fraud Detection",
    "section": "",
    "text": "Oulier Detection\nIntroduction\nA classic example to demostrate anamoly and oultier detection is detection of credit card fraud. In the era of digital transactions, credit card fraud has become a prevalent issue. To tackle this problem, machine learning techniques can be employed for anomaly detection.\nThe Dataset The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n\ndata = pd.read_csv('creditcard.csv',sep=',')\ndata.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\ndata.isnull().values.any()\n\nFalse\n\n\n\n# Get the Fraud and the normal dataset \nfraud = data[data['Class']==1]\nnormal = data[data['Class']==0]\nprint(normal.shape)\nprint(fraud.shape)\n\n(284315, 31)\n(492, 31)\n\n\nPre Processing\nLet us generate a plot to see the Amount of Transactions per Class.\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Valid')\nplt.xlabel('Amount')\nplt.ylabel('Transactions')\nplt.yscale('log')\nplt.show()\n\n\n\n\nLet us generate a plot to visualize time of transaction vs amount per class\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()\n\n\n\n\n\nFraud = data[data['Class']==1]\nValid = data[data['Class']==0]\noutlier_fraction = len(Fraud)/float(len(Valid))\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\nprint(\"Valid Cases : {}\".format(len(Valid)))\nprint(outlier_fraction)\n\nFraud Cases : 492\nValid Cases : 284315\n0.0017304750013189597\n\n\nAfter running this code, we can see that there are 492 fraud cases and they account for 0.0017 of the transactions.\nLet us generate correlation of each feature.\n\nimport seaborn as sns\ncor1 = data.corr()\nfeatures = cor1.index\nplt.figure(figsize=(20,10))\ng=sns.heatmap(data[features].corr(),annot=True,cmap=\"Blues\")\n\n\n\n\n\ncolumns = data.columns.tolist()\ncolumns = [c for c in columns if c not in [\"Class\"]]\ntarget = \"Class\"\nstate = np.random.RandomState(42)\nX = data[columns]\nY = data[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\nprint(X.shape)\nprint(Y.shape)\n\n(284807, 30)\n(284807,)\n\n\nML Models\n\nIsolation Forest The key idea behind Isolation Forest is that anomalies are usually the data points that are easiest to isolate or separate from the normal data. Specifically, an isolation forest uses an ensemble of decision trees, each built in a randomized fashion to isolate various instances in the data. When building each tree, the Isolation Forest algorithm randomly selects a feature and then randomly selects a split value between the maximum and minimum values of that feature. The recursively partitions or splits the data along those lines.\nLOF LOF captures the degree of outliers by measuring the local deviation of a point compared to its neighbors. Points that stand out with respect to their local neighborhoods will have much higher LOF scores, marking them as potential anomalies. A key advantage of LOF is it does not require any assumptions about the underlying data distribution or definitions of global parameters that define outliers. It relies entirely on local neighborhoods.\n\nThe code is as follows:\n\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction)\n   \n}\n\n\nn_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))\n\nIsolation Forest: 675\nAccuracy Score :\n0.9976299739823811\nClassification Report :\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.31      0.32      0.31       492\n\n    accuracy                           1.00    284807\n   macro avg       0.66      0.66      0.66    284807\nweighted avg       1.00      1.00      1.00    284807\n\nLocal Outlier Factor: 935\nAccuracy Score :\n0.9967170750718908\nClassification Report :\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.05      0.05      0.05       492\n\n    accuracy                           1.00    284807\n   macro avg       0.52      0.52      0.52    284807\nweighted avg       1.00      1.00      1.00    284807\n\n\n\nIn this plot, each point on the x-axis represents a transaction, and the y-axis represents the outlier score. The color of the marker indicates whether the transaction was classified as a valid (blue) or fraudulent (red) transaction.\n\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(scores_prediction)), scores_prediction, c=y_pred, cmap='coolwarm', marker='x')\nplt.colorbar(label='Outlier Prediction')\nplt.title('Outlier Scores for Valid and Fraud Transactions')\nplt.xlabel('Transaction Index')\nplt.ylabel('Outlier Score')\nplt.show()\n\n\n\n\nConclusion\nIsolation Forest has an accuracy score of 0.997 and LOF has an accuracy score of 0.996. The key point of difference is that fraud detection is 31% in isolation forest whereas for LOF it is just 5%. Hence for our purposes and this dataset, isolation forest would be more useful."
  },
  {
    "objectID": "posts/Blog4/index.html",
    "href": "posts/Blog4/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Drug Classification\nIntroduction\nClustering is a fundamental technique in the field of unsupervised machine learning, allowing us to discover hidden patterns and group similar data points together. One popular algorithm for clustering is K-Means, and in this blog post, we’ll walk through a hands-on example using the well-known Iris dataset.\nThe Dataset\nThe Iris dataset is a classic dataset in machine learning, often used for testing and learning purposes. It consists of 150 samples of iris flowers, each belonging to one of three species: setosa, versicolor, or virginica. The features include sepal length, sepal width, petal length, and petal width.\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Load the Iris dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\nStandardization Before applying K-Means, we standardize the features. Standardization ensures that all features contribute equally to the distance computation, preventing one feature from dominating the others due to a difference in scale.\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_iris)\n\nML Models\nK-Means Clustering\nIn our example, we set the number of clusters (K) to 3 since we know that there are three species of iris in the dataset. The algorithm then groups the data points into three clusters based on their similarity.\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(X_scaled)\ny_kmeans = kmeans.predict(X_scaled)\n\nC:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nVisualizing the Data\nThe true power of K-Means becomes apparent when we visualize the clustered data. The scatter plot showcases the distinct clusters, with different colors representing each cluster. Additionally, we mark the cluster centers with black ‘x’ markers.\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75, marker='x')\n\nplt.title('K-Means Clustering')\nplt.xlabel('Standardized Feature 1')\nplt.ylabel('Standardized Feature 2')\n\nplt.figure()\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_iris, s=50, cmap='viridis')\nplt.title('True Labels')\nplt.xlabel('Standardized Feature 1')\nplt.ylabel('Standardized Feature 2')\n\nplt.show()"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Salary Patterns with Linear Regression\nIntroduction In this blog post, we’ll look into a salary dataset and employ linear regression to understand the relationship between years of experience and salary. Linear regression in machine learning is a supervised learning algorithm used for predicting a continuous outcome variable based on one or more input features. It establishes a linear relationship between the input features and the target variable by finding the best-fitting line through the data points.\nExploring the Dataset Our dataset contains 30 entries of our two variables, Years of experience and Salary. Let’s load the dataset.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\n\ndf=pd.read_csv('Salary_dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYearsExperience\nSalary\n\n\n\n\n0\n0\n1.2\n39344.0\n\n\n1\n1\n1.4\n46206.0\n\n\n2\n2\n1.6\n37732.0\n\n\n3\n3\n2.1\n43526.0\n\n\n4\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(30, 3)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30 entries, 0 to 29\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       30 non-null     int64  \n 1   YearsExperience  30 non-null     float64\n 2   Salary           30 non-null     float64\ndtypes: float64(2), int64(1)\nmemory usage: 852.0 bytes\n\n\nWe’ll clean our data by dropping unnecessary columns and checking for missing or duplicated values.\n\ndf.drop('Unnamed: 0',axis=1,inplace=True)\n\n\n# Check for Null values\ndf.isna().sum()\n\nYearsExperience    0\nSalary             0\ndtype: int64\n\n\n\n# Check for Duplicated values\ndf.duplicated().sum()\n\n0\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nYearsExperience\nSalary\n\n\n\n\nYearsExperience\n1.000000\n0.978242\n\n\nSalary\n0.978242\n1.000000\n\n\n\n\n\n\n\nEDA\nTo gain insights into the dataset, we’ll compute correlations and create visualizations, such as pair plots and scatter plots.\n\nsns.set(font_scale=2)\nsns.pairplot(df, height=8, aspect=10/8)\n\n\n\n\n\nx=df['YearsExperience']\ny=df['Salary']\n\n\nplt.rcParams[\"figure.figsize\"] = (10, 8)\nplt.scatter(x, y)\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.title(\"Salary with Years of Experience\")\nplt.grid(True)\nplt.show()\nx = np.array(x).reshape(-1, 1)\ny = np.array(y).reshape(-1, 1)\n\n\n\n\nTrain Test Split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15)\n\nLinear Regression Model\n\nlr=LinearRegression()\nlr.fit(x_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nPredictions of our Model\n\ny_preds=lr.predict(x_test)\nprint(y_preds)\n\n[[ 99090.2350146 ]\n [ 91772.22555119]\n [106408.244478  ]\n [ 63414.9388805 ]\n [101834.48856337]]\n\n\n\nplt.rcParams[\"figure.figsize\"] = (10, 8)\nplt.scatter(x, y)\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.plot(x_test, y_preds, color = \"r\", label = \"Regression line\")\nplt.title(\"Salary with Years of Experience\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\nModel Evaluation\n\nprint(\"Mean Absolute Error = \", str(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Squared Error = \", str(mean_squared_error(y_test, y_preds)))\nprint(\"R-squared = \", str(r2_score(y_test, y_preds)))\n\nMean Absolute Error =  6034.749054666276\nMean Squared Error =  48297858.65289231\nR-squared =  0.882503117707831\n\n\nConclusion\nIn conclusion, our exploration into the salary dataset using linear regression has provided valuable insights into the relationship between years of experience and salary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Blog",
    "section": "",
    "text": "Blog-1: Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\n  \n\n\n\n\nK-Means Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nNon Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nCredit Card Fraud Detection\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Shreyas Harathi. I’m currently pursuing my Master’s in Computer Engineering at Virginia Tech\nContact: Email: shreyasharathi00@vt.edu Phone No: 412-482-0336"
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Blog-1: Probability theory and random variables",
    "section": "",
    "text": "Using Naive Bayes To Predict Poisonous Mushrooms\nIntroduction\nTopic: Probability theory and random variables\nIn this blog post, we are going to predict if a mushroom is poisonous or not. We are going to apply naive bayes method, which is based on the bayes theorem in probability.\nImport Libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nThe Dataset\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms in 1981. Each species can be identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. So there 2 classes. Poisonous and edible.\n\ndf = pd.read_csv('mushrooms.csv')\ndf.head()\n\n\n\n\n\n\n\n\nclass\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\ngill-attachment\ngill-spacing\ngill-size\ngill-color\n...\nstalk-surface-below-ring\nstalk-color-above-ring\nstalk-color-below-ring\nveil-type\nveil-color\nring-number\nring-type\nspore-print-color\npopulation\nhabitat\n\n\n\n\n0\np\nx\ns\nn\nt\np\nf\nc\nn\nk\n...\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n1\ne\nx\ns\ny\nt\na\nf\nc\nb\nk\n...\ns\nw\nw\np\nw\no\np\nn\nn\ng\n\n\n2\ne\nb\ns\nw\nt\nl\nf\nc\nb\nn\n...\ns\nw\nw\np\nw\no\np\nn\nn\nm\n\n\n3\np\nx\ny\nw\nt\np\nf\nc\nn\nn\n...\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n4\ne\nx\ns\ng\nf\nn\nf\nw\nb\nk\n...\ns\nw\nw\np\nw\no\ne\nn\na\ng\n\n\n\n\n5 rows × 23 columns\n\n\n\nLet us now visualize the dataset as a simple pie chart.\n\nplt.figure(figsize = (6,6))\nplt.pie(df['class'].value_counts(), startangle = 90, autopct = '%.1f', labels = ['Edible', 'Poisonous'], shadow = True)\nplt.show()\n\n\n\n\nSet dependent and independent variables.\n\nX = df.iloc[:, 1:].values\ny = df.iloc[:, 0].values\n\nCheck if there is null value.\n\ndf.notnull().all()\n\nclass                       True\ncap-shape                   True\ncap-surface                 True\ncap-color                   True\nbruises                     True\nodor                        True\ngill-attachment             True\ngill-spacing                True\ngill-size                   True\ngill-color                  True\nstalk-shape                 True\nstalk-root                  True\nstalk-surface-above-ring    True\nstalk-surface-below-ring    True\nstalk-color-above-ring      True\nstalk-color-below-ring      True\nveil-type                   True\nveil-color                  True\nring-number                 True\nring-type                   True\nspore-print-color           True\npopulation                  True\nhabitat                     True\ndtype: bool\n\n\nAll columns are object. So we need to encode them. I am going to use Label Encoding for this. Encode Independent Variable\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX = df.iloc[:, 1:].apply(le.fit_transform).values\n\nEncode Dependent Variable\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = np.array(le.fit_transform(y))\ny.reshape(len(y), 1)\n\narray([[1],\n       [0],\n       [0],\n       ...,\n       [0],\n       [1],\n       [0]])\n\n\nTrain Test Split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nML MODEL\nFeature Scaling\nBefore we apply the GaussianNB method, we need to scale our data. Differences in the scales across input variables may increase the difficulty of the problem being modeled. So I am going to apply standard scaling. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nNow we can go ahead and try to fit out data in out model.\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\ny_prob = classifier.predict_proba(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\naccuracy_nb = accuracy_score(y_test, y_pred)\nprint('Accuracy is:', accuracy_nb)\n\nAccuracy is: 0.9218461538461539\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred, normalize='true')\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='GnBu', xticklabels=['Edible', 'Poisonous'], yticklabels=['Edible', 'Poisonous'])\nplt.title('Normalized Confusion Matrix of Mushroom Dataset')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n\n\nDistribution\nThe resulting plot provides insights into how confident the Naive Bayes classifier is when predicting whether a mushroom is poisonous. The x-axis represents the predicted probabilities, and the y-axis represents the frequency of occurrences at each probability level.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(y_prob[:, 1], kde=True)\nplt.title('Distribution of Predicted Probabilities for Poisonous Mushrooms')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nConclusion\nIn this blog post, we explored the GaussianNB model and achieved a score of 0.92 on our model. I do believe that other techniques might be more accurate in making the same predictions. I will update this blog when i try them out."
  },
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Classification",
    "section": "",
    "text": "Drug Classification\nIntroduction In this blog post, we shall delve into a machine learning project that concentrates on classifying drugs according to patient features. The dataset assigned for this task encapsulates patients’ information and the prescribed medications. To predict the suitable drug for a patient, our objective entails constructing and assessing two classification models: Logistic Regression and Random Forest. Exploring the Dataset\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.metrics import  classification_report , accuracy_score , confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom category_encoders import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nOur dataset contains details of 200 rows and 6 columns. Each column is a variable, and the variables are : Age, Sex, Blood Pressure, Cholestrol, The ratio of Sodium to Potassium in the Blood and finally, the drug type. This can be verified by running\n\ndf = pd.read_csv(\"drug200.csv\")\ndf\n\n\n\n\n\n\n\n\nAge\nSex\nBP\nCholesterol\nNa_to_K\nDrug\n\n\n\n\n0\n23\nF\nHIGH\nHIGH\n25.355\nDrugY\n\n\n1\n47\nM\nLOW\nHIGH\n13.093\ndrugC\n\n\n2\n47\nM\nLOW\nHIGH\n10.114\ndrugC\n\n\n3\n28\nF\nNORMAL\nHIGH\n7.798\ndrugX\n\n\n4\n61\nF\nLOW\nHIGH\n18.043\nDrugY\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n195\n56\nF\nLOW\nHIGH\n11.567\ndrugC\n\n\n196\n16\nM\nLOW\nHIGH\n12.006\ndrugC\n\n\n197\n52\nM\nNORMAL\nHIGH\n9.894\ndrugX\n\n\n198\n23\nM\nNORMAL\nNORMAL\n14.020\ndrugX\n\n\n199\n40\nF\nLOW\nNORMAL\n11.349\ndrugX\n\n\n\n\n200 rows × 6 columns\n\n\n\nData Preprocessing: We can check for duplicates in the dataset using df.duplicated().sum() here, the output is 0, implying that there are no duplicates.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Age          200 non-null    int64  \n 1   Sex          200 non-null    object \n 2   BP           200 non-null    object \n 3   Cholesterol  200 non-null    object \n 4   Na_to_K      200 non-null    float64\n 5   Drug         200 non-null    object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 9.5+ KB\n\n\n\ndf.duplicated().sum()\n\n0\n\n\nFurther, we can generate plots to visualize the distribution of each target varible. For example, we can see the count of each drug using:\n\nimport seaborn as sns\nsns.countplot(x=df['Drug'])\nplt.title('Drug Distribution');\n\n\n\n\n\nsns.distplot(df['Age']);\n\n\n\n\n\nsns.distplot(df['Na_to_K']);\n\n\n\n\n\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(data=df ,x= df['Sex'] , palette='rocket')\nplt.xlabel('Gender (F=female , M=male)')\nplt.ylabel('Total')\nplt.title('Gender distribution');\n\n\n\n\n\nsns.countplot(y=df['BP'], data=df , palette=\"crest\")\nplt.ylabel('Blood Pressure')\nplt.xlabel('Total')\nplt.title('Blood Pressure Distribution');\n\n\n\n\n\nsns.set_theme(style=\"ticks\")\nsns.scatterplot(x=df.Age[df.Sex=='F'], y=df.Na_to_K[(df.Sex=='F')], c=\"Red\" , )\nsns.scatterplot(x=df.Age[df.Sex=='M'], y=df.Na_to_K[(df.Sex=='M')], c=\"Blue\")\nplt.title('Scattering the Na_to_K based on Age')\nplt.legend([\"Female\", \"Male\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Na_to_K\");\nplt.grid(visible=False)\n\n\n\n\nDefine features (X) and target (y)\n\nX = df.drop('Drug' , axis='columns')\ny = df['Drug']\n\n\nX = pd.get_dummies(X)\n\nSplitting\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , shuffle= True ,random_state=42)\n\nMachine Learning Models\n\nLogistic Regression\n\n\nlr_model = LogisticRegression(max_iter=1500)\nlr_model.fit(X_train , y_train)\nlr_model.score(X_train , y_train)\nlr_pred = lr_model.predict(X_test)\n\n\naccuracy_score(y_test , lr_pred)\nprint(classification_report(y_test , lr_pred))\n\n              precision    recall  f1-score   support\n\n       DrugY       1.00      1.00      1.00        15\n       drugA       1.00      1.00      1.00         6\n       drugB       1.00      1.00      1.00         3\n       drugC       1.00      1.00      1.00         5\n       drugX       1.00      1.00      1.00        11\n\n    accuracy                           1.00        40\n   macro avg       1.00      1.00      1.00        40\nweighted avg       1.00      1.00      1.00        40\n\n\n\n\nsns.heatmap(confusion_matrix(y_test , lr_pred));\n\n\n\n\n\nRandom Forest\n\n\nRF_model = RandomForestClassifier(n_estimators= 200)\nRF_model.fit(X_train , y_train)\nRF_model.score(X_train , y_train)\nRF_pred = RF_model.predict(X_test)\n\n\naccuracy_score(y_test , RF_pred)\nprint(classification_report(y_test , RF_pred))\n\n              precision    recall  f1-score   support\n\n       DrugY       1.00      1.00      1.00        15\n       drugA       1.00      1.00      1.00         6\n       drugB       1.00      1.00      1.00         3\n       drugC       1.00      1.00      1.00         5\n       drugX       1.00      1.00      1.00        11\n\n    accuracy                           1.00        40\n   macro avg       1.00      1.00      1.00        40\nweighted avg       1.00      1.00      1.00        40\n\n\n\n\nsns.heatmap(confusion_matrix(y_test , RF_pred));\n\n\n\n\nHighest Influencing Features\n\nfeatures = X_test.columns\nimportance_f = lr_model.coef_[0]\nfeat_imp = pd.Series(importance_f , index=features).sort_values()\nfeat_imp.tail().plot(kind= 'barh')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importance\");\n\n\n\n\nConclusion\nAt last i used 2 models logitic regression model and random forest model both got 100% accuracy in both training and testing"
  },
  {
    "objectID": "posts/Blog5/index.html",
    "href": "posts/Blog5/index.html",
    "title": "Non Linear Regression",
    "section": "",
    "text": "Exploring Housing Data and Regression Models\nIntroduction\nIn this blog post, we’ll walk through housing data and apply regression models to predict median house values. Nonlinear regression is a statistical method used to model the relationship between variables through nonlinear functions, allowing for more flexible and complex representations than linear regression. It involves estimating parameters that minimize the difference between observed and predicted values, often requiring optimization techniques.\nThe Dataset Our dataset contains 20640 observations and 9 features and 1 target variable(median_house_value). Let’s load the dataset.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\ndata = pd.read_csv(\"housing.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\n\ndata.shape\n\n(20640, 10)\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nPre Processing\nTo gain insights into the dataset, lets preprocess the data by dropping unnecessary columns and create a scatter plot to visualize the relationship between median income and median house value.\n\ndata = data.drop([\"housing_median_age\",\"households\",\"total_bedrooms\",\"longitude\",\"latitude\",\"total_rooms\",\"population\",\"ocean_proximity\"], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nmedian_income\nmedian_house_value\n\n\n\n\n0\n8.3252\n452600.0\n\n\n1\n8.3014\n358500.0\n\n\n2\n7.2574\n352100.0\n\n\n3\n5.6431\n341300.0\n\n\n4\n3.8462\n342200.0\n\n\n\n\n\n\n\n\nX = data.drop(\"median_house_value\", axis=1)\ny = data[\"median_house_value\"]\n\n\nplt.scatter(X, y, alpha=0.5)\nplt.title('Scatter plot')\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\nplt.show()\n\n\n\n\nUsing this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house. There is somewhat positive linear relationship between them.\nSplit the data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nML Models\n1.Linear Regression\nNow, it’s time to build our linear regression model. We’ll split the data into training and testing sets, create the model, and fit it to the training data.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Model initialization\nregression_model = LinearRegression()\n\n# Fit the data(train the model)\nregression_model.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Predict\ny_predicted = regression_model.predict(X_test)\n\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84941.05152406936\nR2 score:  0.4466846804895944\n\n\nWe obtain a R2 score of 0.446.\nThe plot of simple linear regression :\n\n# data points\nplt.scatter(X_train, y_train, s=10)\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n# predicted values\nplt.plot(X_test, y_predicted, color='r')\nplt.show()\n\n\n\n\nResidual plot from linear regression\n\n# Assuming y_predicted and y_test are defined and are numpy arrays or similar data structures that support arithmetic operations\nresidual = y_test - y_predicted\n\n\n# Corrected call to residplot\nsns.residplot(x=y_predicted, y=residual, lowess=True, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()\n\n\n\n\nThe residuals exhibit a clear non straight line, which provides a strong indication of non-linearity in the data\nThis makes us to do somethhing more to find better fit of the model.\nModel 2:\nApplying transformation\n\ntf = np.sqrt(X_train) \ntf1 = np.sqrt(X_test)\n\n\nplt.scatter(tf, y_train)\nplt.show()\n\n\n\n\nFitting a model\n\nregression_model.fit(tf, y_train)\n# Predict\ny_predicted = regression_model.predict(tf1)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [175550.81872626]\nIntercept: -129097.46805625176\nRoot mean squared error:  85566.3672277501\nR2 score:  0.4385079432151452\n\n\nThe obtained R2 score is 0.4385 which is worse than the previous model.\nResidual plot for transformed\n\n# Assuming y_predicted and y_test are defined and are numpy arrays or similar data structures that support arithmetic operations\nresidual = y_test - y_predicted\n\n\n# Corrected call to residplot\nsns.residplot(x=y_predicted, y=residual, lowess=True, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()\n\n\n\n\nResidual plot for the transformed linear regression is more zigzag than the simple linear regression. This residual plot suggest that transformation makes the relationship more non- linear in nature.\nModel 3:\nWe will now attempt a polynomial regression model.\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ndef viz_polymonial():\n    plt.scatter(X_train, y_train, color=\"red\")\n    plt.plot(X_train, pol_reg.predict(poly_reg.fit_transform(X_train)))\n    plt.xlabel('median_income')\n    plt.ylabel('median_house_value')\n    plt.show()\n    return\nviz_polymonial()\n\n\n\n\n\n# Predict\nX_p = poly_reg.fit_transform(X_test)\ny_predicted = pol_reg.predict(X_p)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [175550.81872626]\nIntercept: -129097.46805625176\nRoot mean squared error:  84699.90676455045\nR2 score:  0.44982190770645947\n\n\nWe end up with a R2 of 0.449, which is our best score yet. now we run:\n\n# Assuming y_predicted and y_test are defined and are numpy arrays or similar data structures that support arithmetic operations\nresidual = y_test - y_predicted\n\n# Corrected call to residplot\nsns.residplot(x=y_predicted, y=residual, lowess=True, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.show()\n\n\n\n\nConclusion\nModel 3 has the best R2 score but it is also far more complex than model 1. This trade off must be considered while selecting a suitable model."
  }
]