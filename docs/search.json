[
  {
    "objectID": "posts/Blog3/index.html",
    "href": "posts/Blog3/index.html",
    "title": "Classification",
    "section": "",
    "text": "Drug Classification\nIntroduction In this blog post, we shall delve into a machine learning project that concentrates on classifying drugs according to patient features. The dataset assigned for this task encapsulates patients’ information and the prescribed medications. To predict the suitable drug for a patient, our objective entails constructing and assessing two classification models: Logistic Regression and Random Forest. Exploring the Dataset\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.metrics import  classification_report , accuracy_score , confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom category_encoders import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nOur dataset contains details of 200 rows and 6 columns. Each column is a variable, and the variables are : Age, Sex, Blood Pressure, Cholestrol, The ratio of Sodium to Potassium in the Blood and finally, the drug type. This can be verified by running\n\ndf = pd.read_csv(\"drug200.csv\")\ndf\n\n\n\n\n\n\n\n\nAge\nSex\nBP\nCholesterol\nNa_to_K\nDrug\n\n\n\n\n0\n23\nF\nHIGH\nHIGH\n25.355\nDrugY\n\n\n1\n47\nM\nLOW\nHIGH\n13.093\ndrugC\n\n\n2\n47\nM\nLOW\nHIGH\n10.114\ndrugC\n\n\n3\n28\nF\nNORMAL\nHIGH\n7.798\ndrugX\n\n\n4\n61\nF\nLOW\nHIGH\n18.043\nDrugY\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n195\n56\nF\nLOW\nHIGH\n11.567\ndrugC\n\n\n196\n16\nM\nLOW\nHIGH\n12.006\ndrugC\n\n\n197\n52\nM\nNORMAL\nHIGH\n9.894\ndrugX\n\n\n198\n23\nM\nNORMAL\nNORMAL\n14.020\ndrugX\n\n\n199\n40\nF\nLOW\nNORMAL\n11.349\ndrugX\n\n\n\n\n200 rows × 6 columns\n\n\n\nData Preprocessing: We can check for duplicates in the dataset using df.duplicated().sum() here, the output is 0, implying that there are no duplicates.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Age          200 non-null    int64  \n 1   Sex          200 non-null    object \n 2   BP           200 non-null    object \n 3   Cholesterol  200 non-null    object \n 4   Na_to_K      200 non-null    float64\n 5   Drug         200 non-null    object \ndtypes: float64(1), int64(1), object(4)\nmemory usage: 9.5+ KB\n\n\n\ndf.duplicated().sum()\n\n0\n\n\nFurther, we can generate plots to visualize the distribution of each target varible. For example, we can see the count of each drug using:\n\nimport seaborn as sns\nsns.countplot(x=df['Drug'])\nplt.title('Drug Distribution');\n\n\n\n\n\nsns.distplot(df['Age']);\n\n\n\n\n\nsns.distplot(df['Na_to_K']);\n\n\n\n\n\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(data=df ,x= df['Sex'] , palette='rocket')\nplt.xlabel('Gender (F=female , M=male)')\nplt.ylabel('Total')\nplt.title('Gender distribution');\n\n\n\n\n\nsns.countplot(y=df['BP'], data=df , palette=\"crest\")\nplt.ylabel('Blood Pressure')\nplt.xlabel('Total')\nplt.title('Blood Pressure Distribution');\n\n\n\n\n\nsns.set_theme(style=\"ticks\")\nsns.scatterplot(x=df.Age[df.Sex=='F'], y=df.Na_to_K[(df.Sex=='F')], c=\"Red\" , )\nsns.scatterplot(x=df.Age[df.Sex=='M'], y=df.Na_to_K[(df.Sex=='M')], c=\"Blue\")\nplt.title('Scattering the Na_to_K based on Age')\nplt.legend([\"Female\", \"Male\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Na_to_K\");\nplt.grid(visible=False)\n\n\n\n\nDefine features (X) and target (y)\n\nX = df.drop('Drug' , axis='columns')\ny = df['Drug']\n\n\nX = pd.get_dummies(X)\n\nSplitting\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , shuffle= True ,random_state=42)\n\nMachine Learning Models\n\nLogistic Regression\n\n\nlr_model = LogisticRegression(max_iter=1500)\nlr_model.fit(X_train , y_train)\nlr_model.score(X_train , y_train)\nlr_pred = lr_model.predict(X_test)\n\n\naccuracy_score(y_test , lr_pred)\nprint(classification_report(y_test , lr_pred))\n\n              precision    recall  f1-score   support\n\n       DrugY       1.00      1.00      1.00        15\n       drugA       1.00      1.00      1.00         6\n       drugB       1.00      1.00      1.00         3\n       drugC       1.00      1.00      1.00         5\n       drugX       1.00      1.00      1.00        11\n\n    accuracy                           1.00        40\n   macro avg       1.00      1.00      1.00        40\nweighted avg       1.00      1.00      1.00        40\n\n\n\n\nsns.heatmap(confusion_matrix(y_test , lr_pred));\n\n\n\n\n\nRandom Forest\n\n\nRF_model = RandomForestClassifier(n_estimators= 200)\nRF_model.fit(X_train , y_train)\nRF_model.score(X_train , y_train)\nRF_pred = RF_model.predict(X_test)\n\n\naccuracy_score(y_test , RF_pred)\nprint(classification_report(y_test , RF_pred))\n\n              precision    recall  f1-score   support\n\n       DrugY       1.00      1.00      1.00        15\n       drugA       1.00      1.00      1.00         6\n       drugB       1.00      1.00      1.00         3\n       drugC       1.00      1.00      1.00         5\n       drugX       1.00      1.00      1.00        11\n\n    accuracy                           1.00        40\n   macro avg       1.00      1.00      1.00        40\nweighted avg       1.00      1.00      1.00        40\n\n\n\n\nsns.heatmap(confusion_matrix(y_test , RF_pred));\n\n\n\n\nHighest Influencing Features\n\nfeatures = X_test.columns\nimportance_f = lr_model.coef_[0]\nfeat_imp = pd.Series(importance_f , index=features).sort_values()\nfeat_imp.tail().plot(kind= 'barh')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importance\");\n\n\n\n\nConclusion\nAt last i used 2 models logitic regression model and random forest model both got 100% accuracy in both training and testing"
  },
  {
    "objectID": "posts/Blog1/index.html",
    "href": "posts/Blog1/index.html",
    "title": "Blog-1: Probability theory and random variables",
    "section": "",
    "text": "Using Naive Bayes To Predict Poisonous Mushrooms\nIntroduction\nTopic: Probability theory and random variables\nIn this blog post, we are going to predict if a mushroom is poisonous or not. We are going to apply naive bayes method, which is based on the bayes theorem in probability.\nImport Libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nThe Dataset\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms in 1981. Each species can be identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. So there 2 classes. Poisonous and edible.\n\ndf = pd.read_csv('mushrooms.csv')\ndf.head()\n\n\n\n\n\n\n\n\nclass\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\ngill-attachment\ngill-spacing\ngill-size\ngill-color\n...\nstalk-surface-below-ring\nstalk-color-above-ring\nstalk-color-below-ring\nveil-type\nveil-color\nring-number\nring-type\nspore-print-color\npopulation\nhabitat\n\n\n\n\n0\np\nx\ns\nn\nt\np\nf\nc\nn\nk\n...\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n1\ne\nx\ns\ny\nt\na\nf\nc\nb\nk\n...\ns\nw\nw\np\nw\no\np\nn\nn\ng\n\n\n2\ne\nb\ns\nw\nt\nl\nf\nc\nb\nn\n...\ns\nw\nw\np\nw\no\np\nn\nn\nm\n\n\n3\np\nx\ny\nw\nt\np\nf\nc\nn\nn\n...\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n4\ne\nx\ns\ng\nf\nn\nf\nw\nb\nk\n...\ns\nw\nw\np\nw\no\ne\nn\na\ng\n\n\n\n\n5 rows × 23 columns\n\n\n\nLet us now visualize the dataset as a simple pie chart.\n\nplt.figure(figsize = (6,6))\nplt.pie(df['class'].value_counts(), startangle = 90, autopct = '%.1f', labels = ['Edible', 'Poisonous'], shadow = True)\nplt.show()\n\n\n\n\nSet dependent and independent variables.\n\nX = df.iloc[:, 1:].values\ny = df.iloc[:, 0].values\n\nCheck if there is null value.\n\ndf.notnull().all()\n\nclass                       True\ncap-shape                   True\ncap-surface                 True\ncap-color                   True\nbruises                     True\nodor                        True\ngill-attachment             True\ngill-spacing                True\ngill-size                   True\ngill-color                  True\nstalk-shape                 True\nstalk-root                  True\nstalk-surface-above-ring    True\nstalk-surface-below-ring    True\nstalk-color-above-ring      True\nstalk-color-below-ring      True\nveil-type                   True\nveil-color                  True\nring-number                 True\nring-type                   True\nspore-print-color           True\npopulation                  True\nhabitat                     True\ndtype: bool\n\n\nAll columns are object. So we need to encode them. I am going to use Label Encoding for this. Encode Independent Variable\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX = df.iloc[:, 1:].apply(le.fit_transform).values\n\nEncode Dependent Variable\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = np.array(le.fit_transform(y))\ny.reshape(len(y), 1)\n\narray([[1],\n       [0],\n       [0],\n       ...,\n       [0],\n       [1],\n       [0]])\n\n\nTrain Test Split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nML MODEL\nFeature Scaling\nBefore we apply the GaussianNB method, we need to scale our data. Differences in the scales across input variables may increase the difficulty of the problem being modeled. So I am going to apply standard scaling. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nNow we can go ahead and try to fit out data in out model.\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\ny_prob = classifier.predict_proba(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\naccuracy_nb = accuracy_score(y_test, y_pred)\nprint('Accuracy is:', accuracy_nb)\n\nAccuracy is: 0.9218461538461539\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred, normalize='true')\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap='GnBu', xticklabels=['Edible', 'Poisonous'], yticklabels=['Edible', 'Poisonous'])\nplt.title('Normalized Confusion Matrix of Mushroom Dataset')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n\n\nDistribution\nThe resulting plot provides insights into how confident the Naive Bayes classifier is when predicting whether a mushroom is poisonous. The x-axis represents the predicted probabilities, and the y-axis represents the frequency of occurrences at each probability level.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(y_prob[:, 1], kde=True)\nplt.title('Distribution of Predicted Probabilities for Poisonous Mushrooms')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nConclusion\nIn this blog post, we explored the GaussianNB model and achieved a score of 0.92 on our model. I do believe that other techniques might be more accurate in making the same predictions. I will update this blog when i try them out."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Amal Sujith (ID: 906578048) student of MS in Computer Engineering. I’m currently working on Machine Learning and improving my problem solving skills.\nContact: Email: amalsmenon7@gmail.com Phone No: +918281725367"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Blog",
    "section": "",
    "text": "Blog-1: Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nShreyas Harathi\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nAmal Sujith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog2/index.html",
    "href": "posts/Blog2/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Salary Patterns with Linear Regression\nIntroduction In this blog post, we’ll look into a salary dataset and employ linear regression to understand the relationship between years of experience and salary. Linear regression in machine learning is a supervised learning algorithm used for predicting a continuous outcome variable based on one or more input features. It establishes a linear relationship between the input features and the target variable by finding the best-fitting line through the data points.\nExploring the Dataset Our dataset contains 30 entries of our two variables, Years of experience and Salary. Let’s load the dataset.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\n\ndf=pd.read_csv('Salary_dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYearsExperience\nSalary\n\n\n\n\n0\n0\n1.2\n39344.0\n\n\n1\n1\n1.4\n46206.0\n\n\n2\n2\n1.6\n37732.0\n\n\n3\n3\n2.1\n43526.0\n\n\n4\n4\n2.3\n39892.0\n\n\n\n\n\n\n\n\ndf.shape\n\n(30, 3)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30 entries, 0 to 29\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       30 non-null     int64  \n 1   YearsExperience  30 non-null     float64\n 2   Salary           30 non-null     float64\ndtypes: float64(2), int64(1)\nmemory usage: 852.0 bytes\n\n\nWe’ll clean our data by dropping unnecessary columns and checking for missing or duplicated values.\n\ndf.drop('Unnamed: 0',axis=1,inplace=True)\n\n\n# Check for Null values\ndf.isna().sum()\n\nYearsExperience    0\nSalary             0\ndtype: int64\n\n\n\n# Check for Duplicated values\ndf.duplicated().sum()\n\n0\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nYearsExperience\nSalary\n\n\n\n\nYearsExperience\n1.000000\n0.978242\n\n\nSalary\n0.978242\n1.000000\n\n\n\n\n\n\n\nEDA\nTo gain insights into the dataset, we’ll compute correlations and create visualizations, such as pair plots and scatter plots.\n\nsns.set(font_scale=2)\nsns.pairplot(df, height=8, aspect=10/8)\n\n\n\n\n\nx=df['YearsExperience']\ny=df['Salary']\n\n\nplt.rcParams[\"figure.figsize\"] = (10, 8)\nplt.scatter(x, y)\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.title(\"Salary with Years of Experience\")\nplt.grid(True)\nplt.show()\nx = np.array(x).reshape(-1, 1)\ny = np.array(y).reshape(-1, 1)\n\n\n\n\nTrain Test Split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15)\n\nLinear Regression Model\n\nlr=LinearRegression()\nlr.fit(x_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nPredictions of our Model\n\ny_preds=lr.predict(x_test)\nprint(y_preds)\n\n[[117317.85222322]\n [ 44763.57439382]\n [ 72448.75935504]\n [ 62902.14385117]\n [124000.48307593]]\n\n\n\nplt.rcParams[\"figure.figsize\"] = (10, 8)\nplt.scatter(x, y)\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.plot(x_test, y_preds, color = \"r\", label = \"Regression line\")\nplt.title(\"Salary with Years of Experience\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\nModel Evaluation\n\nprint(\"Mean Absolute Error = \", str(mean_absolute_error(y_test, y_preds)))\nprint(\"Mean Squared Error = \", str(mean_squared_error(y_test, y_preds)))\nprint(\"R-squared = \", str(r2_score(y_test, y_preds)))\n\nMean Absolute Error =  2470.9050393693715\nMean Squared Error =  9295375.137068186\nR-squared =  0.989897973894557\n\n\nConclusion\nIn conclusion, our exploration into the salary dataset using linear regression has provided valuable insights into the relationship between years of experience and salary."
  }
]